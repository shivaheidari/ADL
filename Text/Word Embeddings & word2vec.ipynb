{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEfyA4VG2GI5"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://www.ucalgary.ca/themes/ucalgary/ucws_theme/images/UCalgary.svg\" width='30%'>\n",
        "</center>\n",
        "\n",
        "[comment]: <> (The following line is for the TOPIC of the week)\n",
        "<p style=\"text-align:left;\"><font size='4'><b> Introduction to NLP </b></font></p>\n",
        "\n",
        "---\n",
        "\n",
        "# Word Embeddings | word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-fAIPwIq1pd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p1q2gXgq4hW",
        "outputId": "7797411e-c3ff-4f74-aee3-a40a943bd977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-01 19:19:49--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 34.198.1.81\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|34.198.1.81|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "\rtext8.zip             0%[                    ]       0  --.-KB/s               \rtext8.zip           100%[===================>]  29.89M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-10-01 19:19:49 (269 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists(\"text8.zip\"):\n",
        "    !wget http://mattmahoney.net/dc/text8.zip\n",
        "    !unzip text8.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9_fvqz3FrUNq"
      },
      "outputs": [],
      "source": [
        "LIMIT = 100_000\n",
        "with open('text8') as f:\n",
        "    corpus = f.read().split()[:LIMIT]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NidlBo0driIH"
      },
      "source": [
        "# word co-occurrency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiiwV6-Brf81",
        "outputId": "70fe3c99-2eb3-45f7-cb6d-46b14ef760ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12023"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2idx = {}\n",
        "idx2word = []\n",
        "for word in corpus:\n",
        "    if word not in word2idx:\n",
        "        word2idx[word] = len(word2idx)\n",
        "        idx2word.append(word)\n",
        "vocabulary_size = len(word2idx)\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3CrVIHLrwpc",
        "outputId": "c71872ba-667a-4a98-e16b-f4d4d8577a38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 43,   1,  14, ...,   0,   0,   0],\n",
              "       [  1,   0,   1, ...,   0,   0,   0],\n",
              "       [ 14,   1, 165, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,   0,   1,   0],\n",
              "       [  0,   0,   0, ...,   1,   0,   0],\n",
              "       [  0,   0,   0, ...,   0,   1,   0]], dtype=uint32)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "co_occurrency_matrix = np.zeros((vocabulary_size, vocabulary_size), dtype=np.uint32)\n",
        "window = 5\n",
        "for i in range(len(corpus)):\n",
        "    token = corpus[i]\n",
        "    token_idx = word2idx[token]\n",
        "    for j in range(max(i-window, 0), min(i+window, len(corpus))):\n",
        "        if i == j:\n",
        "            continue\n",
        "        neighbor = corpus[j]\n",
        "        neighbor_idx = word2idx[neighbor]\n",
        "        co_occurrency_matrix[token_idx, neighbor_idx] += 1\n",
        "\n",
        "co_occurrency_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zgmrgP8br439"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a,b):\n",
        "    a = a / np.sqrt((a ** 2).sum(-1))\n",
        "    b = b / np.sqrt((b ** 2).sum(-1))\n",
        "    return np.dot(a, b.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxA5JPkmv6qP",
        "outputId": "957184e4-afee-4279-e977-d3160606da7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.13074409009212268"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(co_occurrency_matrix[word2idx['toronto']], co_occurrency_matrix[word2idx['ottawa']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6icNVeMDtu8U",
        "outputId": "fe2c34ba-134a-4d16-ecc3-d9fa7315c5c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5657035277517756"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(co_occurrency_matrix[word2idx['like']], co_occurrency_matrix[word2idx['love']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAQhmOo5t2aB",
        "outputId": "f6d9094e-2d2b-4554-ff16-4585993013d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4624 ph\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# choose a random word\n",
        "widx = np.random.randint(vocabulary_size)\n",
        "print(widx, idx2word[widx])\n",
        "\n",
        "cosine_similarity(co_occurrency_matrix[word2idx['toronto']], co_occurrency_matrix[widx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTqBPRs5qWvT"
      },
      "source": [
        "## From co-occurrency matrix to embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mRJGPgtuQKS"
      },
      "source": [
        "Use PCA to compress the matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJzAxHEPt7OG",
        "outputId": "ac27e74b-cc71-4623-c1ca-c7f3565ba381"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 6.30552646e+01, -1.69815162e+01, -1.61517171e+01, ...,\n",
              "         2.17251888e+00,  1.57627993e+01,  2.55399765e+01],\n",
              "       [-3.10055869e+00, -1.15508323e+00, -9.23283847e-01, ...,\n",
              "         1.05221189e-01, -2.02608554e-01,  4.67699227e-01],\n",
              "       [ 6.61249912e+02, -1.38696833e+02,  3.80169902e+01, ...,\n",
              "        -6.91352478e-01,  1.18812189e+00, -1.15850425e+00],\n",
              "       ...,\n",
              "       [-6.77471712e+00,  7.92928563e-02, -3.89485414e-01, ...,\n",
              "         2.92276088e-01,  1.91066177e-01, -2.45395094e-01],\n",
              "       [-6.84542966e+00,  1.23103711e-01, -3.61984313e-01, ...,\n",
              "         3.59188213e-01,  2.83413948e-01, -4.34373211e-01],\n",
              "       [-6.02542216e+00, -1.69505187e-01,  2.47964510e-01, ...,\n",
              "         2.78817440e-01,  1.92801560e-01, -4.12069765e-01]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_co_occurrency_matrix = PCA(n_components=50).fit_transform(co_occurrency_matrix)\n",
        "\n",
        "pca_co_occurrency_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A9WY2QqrGmOy"
      },
      "outputs": [],
      "source": [
        "# sqrt(average(word_occurency)) / 128, 256, 1024, 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juvXuV3TI24Q",
        "outputId": "bf6f4d7f-9bf1-4bad-e03c-fd259e1c6656"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12023, 50)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pca_co_occurrency_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moeY31_YuZ6t",
        "outputId": "208f8941-47cb-4b4c-a288-814c85fc4fde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.38361762145970774"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(pca_co_occurrency_matrix[word2idx['like']], pca_co_occurrency_matrix[word2idx['love']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TpReVOiTHBlW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLav6ViLuyoj",
        "outputId": "cb1604ec-e607-415c-d730-dac6c62e5b72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.8940410414439164"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(pca_co_occurrency_matrix[word2idx['like']], pca_co_occurrency_matrix[widx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fesw-FbMwDkT"
      },
      "source": [
        "PCA cause loss of information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXz-3WvlvvSL",
        "outputId": "61213bf3-eefc-4435-fe58-573a03d6a605"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8576986916756436"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(pca_co_occurrency_matrix[word2idx['toronto']], pca_co_occurrency_matrix[word2idx['ottawa']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0il6OucwLoa"
      },
      "source": [
        "# word2vec\n",
        "\n",
        "## skip-gram with negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Rrfy4NV5uy6a"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "class SkipGram:\n",
        "    def __init__(self,\n",
        "                 dimension=50,\n",
        "                 window=5,\n",
        "                 epoch=10,\n",
        "                 learning_rate=0.025,\n",
        "                 negative_samples=5,\n",
        "                 corpus=None,):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.word_frequency = []\n",
        "        self.vocabulary_size = 0\n",
        "        self.dimension = dimension\n",
        "        self.window = window\n",
        "        self.epoch = epoch\n",
        "        self.learning_rate = learning_rate\n",
        "        self.negative_samples = negative_samples\n",
        "        if corpus:\n",
        "            self.build_vocab(corpus)\n",
        "            self.init_weights()\n",
        "            self.train(corpus, epoch=self.epoch, learning_rate=self.learning_rate)\n",
        "\n",
        "    def build_vocab(self, corpus):\n",
        "        for word in corpus:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = len(self.word2idx)\n",
        "                self.idx2word.append(word)\n",
        "                self.word_frequency.append(1)\n",
        "            else:\n",
        "                self.word_frequency[self.word2idx[word]] += 1\n",
        "\n",
        "        self.vocabulary_size = len(self.word2idx)\n",
        "\n",
        "    def init_weights(self):\n",
        "        # word_idx, dim of the word\n",
        "        self.word_embeddings = (np.random.random(size=(self.vocabulary_size, self.dimension)) - 0.5) / self.dimension\n",
        "        self.word_output_weights = np.zeros((self.vocabulary_size, self.dimension))\n",
        "        word_frequency = np.array(self.word_frequency)\n",
        "        # create a negative sample draw table\n",
        "        word_frequency = word_frequency ** 0.75\n",
        "        negative_sample_draw_table = word_frequency / word_frequency.sum()\n",
        "        self.negative_sample_draw_table = negative_sample_draw_table.cumsum()\n",
        "\n",
        "\n",
        "    def train(self, corpus, epoch=1, learning_rate=0.025):\n",
        "        # convert word to word_idx\n",
        "        corpus = [self.word2idx[word] for word in corpus if word in self.word2idx]\n",
        "        corpus_size = len(corpus)\n",
        "\n",
        "        samples_counts = 0\n",
        "\n",
        "        for i in range(epoch):\n",
        "            print(f\"epoch {i+1}\")\n",
        "            print(self.word_embeddings[self.word2idx['like']])\n",
        "            print(self.word_output_weights[self.word2idx['like']])\n",
        "            print(\n",
        "                cosine_similarity(\n",
        "                    self.word_embeddings[self.word2idx['like']],\n",
        "                    self.word_embeddings[self.word2idx['love']],\n",
        "                )\n",
        "            )\n",
        "            for i, word_idx in enumerate(corpus):\n",
        "                w = np.random.randint(self.window)\n",
        "                for j in range(max(i-w, 0), min(i+w+1, corpus_size)):\n",
        "                    samples_counts += 1\n",
        "                    # if samples_counts >= 50:\n",
        "                    #     break\n",
        "\n",
        "                    neighbor_idx = corpus[j]\n",
        "                    if i == j:\n",
        "                        continue\n",
        "                    # print(f\"train window={w} {self.idx2word[word_idx]}, {self.idx2word[neighbor_idx]}\")\n",
        "                    self._train_single_pair(word_idx, neighbor_idx, learning_rate)\n",
        "                # learning_rate -= samples_counts * delta\n",
        "\n",
        "    def _train_single_pair(self, i, j, learning_rate):\n",
        "        # vector of dim = d\n",
        "        v_w_i = self.word_embeddings[i].copy()\n",
        "\n",
        "        u_w_j = self.word_output_weights[j].copy()\n",
        "\n",
        "        gridant_v_w_i = np.zeros(self.dimension)\n",
        "\n",
        "        # processing negative sample\n",
        "        for k in self._get_negative_sample():\n",
        "            # skip if negative sample equals to true sample\n",
        "            if k == j:\n",
        "                continue\n",
        "            u_w_k = self.word_output_weights[k].copy()\n",
        "            # gridant from negative sample\n",
        "            gridant_v_w_i += -sigmoid(np.dot(u_w_k,v_w_i))*u_w_k\n",
        "            # update negative sample\n",
        "            self.word_output_weights[k] = u_w_k + learning_rate * (-sigmoid(np.dot(u_w_k,v_w_i))*v_w_i)\n",
        "\n",
        "        # gridant from positive sample\n",
        "        gridant_v_w_i += (1-sigmoid(np.dot(u_w_j,v_w_i)))*u_w_j\n",
        "        # update positive sample\n",
        "        self.word_output_weights[j] = u_w_j + learning_rate * ((1-sigmoid(np.dot(u_w_j,v_w_i)))*v_w_i)\n",
        "\n",
        "        # update word embedding\n",
        "        self.word_embeddings[i] = v_w_i + learning_rate * gridant_v_w_i\n",
        "\n",
        "    def _get_negative_sample(self):\n",
        "        for i in range(self.negative_samples):\n",
        "            yield self.negative_sample_draw_table.searchsorted(np.random.random())\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        assert word in self.word2idx, KeyError(\"word not found in vocabulary\")\n",
        "\n",
        "        return self.word_embeddings[self.word2idx[word]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4my0r5nG265",
        "outputId": "e03d3d57-2984-4d4d-a9ca-5617fd04adf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "[-0.00371853  0.00699322 -0.00612108  0.00866496  0.00634003 -0.00046832\n",
            " -0.00994657  0.0051821   0.00509176  0.00117535 -0.00250108 -0.00506595\n",
            " -0.00884773 -0.00231176  0.00038681 -0.00020532 -0.00906766 -0.00999556\n",
            " -0.00878635 -0.00515008 -0.00222579  0.00513445  0.00586812 -0.00553174\n",
            " -0.0064082   0.00143484 -0.00722414 -0.00851997 -0.00614972  0.00073323\n",
            "  0.00923772  0.00729922 -0.0099276   0.00866795 -0.00901245 -0.00545791\n",
            "  0.00894026  0.00684891 -0.00301517 -0.00090381  0.00627354  0.00794842\n",
            "  0.00891131  0.00247643  0.00725883  0.00738247 -0.00529736  0.00022671\n",
            " -0.00050927  0.00834318]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "-0.12397147970395939\n"
          ]
        }
      ],
      "source": [
        "model = SkipGram(epoch=1, dimension=50, learning_rate=0.05, corpus=corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkur6gqfBoXo",
        "outputId": "fb3999ad-94a1-4337-d1ab-9b5a760ab9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "[ 0.21427154 -0.40711229 -0.48098662 -0.05796207 -0.18346259  0.4251024\n",
            " -0.53778189 -0.61815905  0.20067967  0.24324349 -0.06698374 -0.30328741\n",
            "  0.23284905 -0.17497198  0.3223278   0.43442391  0.35162126 -0.49003714\n",
            "  0.03665783 -0.55578769  0.2805594  -0.29531096  0.38731458  0.63898095\n",
            "  0.18557924  0.10225986 -0.42208882 -0.29941452  0.20785154 -0.06483977\n",
            "  0.02746589  0.33904502  0.58675311  0.29616159  0.14822049 -0.08173149\n",
            " -0.25489478 -0.02230743 -0.4118917   0.0652149  -0.06966938 -0.18134278\n",
            " -0.22621215 -0.10822873  0.27194882  0.16121671 -0.48745452 -0.27932258\n",
            "  0.00101528 -0.34586833]\n",
            "[ 0.02394186  0.19493628  0.14712533  0.16032663  0.06561548 -0.21021657\n",
            "  0.30066553  0.37501663 -0.13518406 -0.08693984  0.0514937   0.22527309\n",
            " -0.07502714  0.21642611 -0.25513767 -0.14703731 -0.14151864  0.15333023\n",
            "  0.08094464  0.23284287 -0.25141465  0.08682712 -0.26900121 -0.15295286\n",
            " -0.13643775  0.0356769   0.28069218  0.10145693 -0.15346338 -0.04908361\n",
            " -0.15617211 -0.117424   -0.35675411 -0.08218917 -0.00286809  0.02815472\n",
            "  0.3066271  -0.07665181  0.31314759 -0.06090812  0.0357588   0.08846655\n",
            "  0.21777938 -0.04585488 -0.17785998 -0.13800925  0.20192597  0.14168283\n",
            "  0.10038937  0.10676948]\n",
            "0.9618842868502082\n"
          ]
        }
      ],
      "source": [
        "model.train(corpus, epoch=1, learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HhFv0h_TXYh",
        "outputId": "65252321-f07b-4bf0-8001-9d46e5e56758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "microbiologist\n",
            "0.04493874684759133\n"
          ]
        }
      ],
      "source": [
        "neighbor_idx = np.random.randint(model.vocabulary_size)\n",
        "neighbor = model.idx2word[neighbor_idx]\n",
        "print(neighbor)\n",
        "print(cosine_similarity(\n",
        "    model['like'],\n",
        "    model[neighbor],\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ5BS1ACaa0C",
        "outputId": "cd09b61d-774c-4bc6-82e4-13e425a944d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.1, 0.2, 0.3, 0.4]), array([0.1, 0.3, 0.6, 1. ]))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freq = np.array([1,2,3,4])\n",
        "freq = freq / freq.sum()\n",
        "freq, freq.cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9vQo6iymapoV",
        "outputId": "d03d3e13-421f-46a9-f85b-e7ed0b276bb5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'regarded'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.word2idx['like']\n",
        "idx = 100\n",
        "model.idx2word[100]\n",
        "\n",
        "# dim = np.mean(model.frequence) ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1n0ZGxqgf6T",
        "outputId": "6f2d4c27-26c8-474d-8318-a90f2f2f150d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['anarchism',\n",
              " 'originated',\n",
              " 'as',\n",
              " 'a',\n",
              " 'term',\n",
              " 'of',\n",
              " 'abuse',\n",
              " 'first',\n",
              " 'used',\n",
              " 'against',\n",
              " 'early',\n",
              " 'working',\n",
              " 'class',\n",
              " 'radicals',\n",
              " 'including',\n",
              " 'the',\n",
              " 'diggers',\n",
              " 'of',\n",
              " 'the',\n",
              " 'english',\n",
              " 'revolution',\n",
              " 'and',\n",
              " 'the',\n",
              " 'sans',\n",
              " 'culottes',\n",
              " 'of',\n",
              " 'the',\n",
              " 'french',\n",
              " 'revolution',\n",
              " 'whilst',\n",
              " 'the',\n",
              " 'term',\n",
              " 'is',\n",
              " 'still',\n",
              " 'used',\n",
              " 'in',\n",
              " 'a',\n",
              " 'pejorative',\n",
              " 'way',\n",
              " 'to',\n",
              " 'describe',\n",
              " 'any',\n",
              " 'act',\n",
              " 'that',\n",
              " 'used',\n",
              " 'violent',\n",
              " 'means',\n",
              " 'to',\n",
              " 'destroy',\n",
              " 'the',\n",
              " 'organization',\n",
              " 'of',\n",
              " 'society',\n",
              " 'it',\n",
              " 'has',\n",
              " 'also',\n",
              " 'been',\n",
              " 'taken',\n",
              " 'up',\n",
              " 'as',\n",
              " 'a',\n",
              " 'positive',\n",
              " 'label',\n",
              " 'by',\n",
              " 'self',\n",
              " 'defined',\n",
              " 'anarchists',\n",
              " 'the',\n",
              " 'word',\n",
              " 'anarchism',\n",
              " 'is',\n",
              " 'derived',\n",
              " 'from',\n",
              " 'the',\n",
              " 'greek',\n",
              " 'without',\n",
              " 'archons',\n",
              " 'ruler',\n",
              " 'chief',\n",
              " 'king',\n",
              " 'anarchism',\n",
              " 'as',\n",
              " 'a',\n",
              " 'political',\n",
              " 'philosophy',\n",
              " 'is',\n",
              " 'the',\n",
              " 'belief',\n",
              " 'that',\n",
              " 'rulers',\n",
              " 'are',\n",
              " 'unnecessary',\n",
              " 'and',\n",
              " 'should',\n",
              " 'be',\n",
              " 'abolished',\n",
              " 'although',\n",
              " 'there',\n",
              " 'are',\n",
              " 'differing']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[:100]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
